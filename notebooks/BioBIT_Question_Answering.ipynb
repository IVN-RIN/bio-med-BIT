{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IVN-RIN/bio-med-BIT/blob/main/notebooks/BioBIT_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm864d-lNgzY"
      },
      "source": [
        "# **BioBIT Fine-Tuning Experiment For <u>Question Answering</u>**\n",
        "\n",
        "*Tommaso M Buonocore, University of Pavia, 2022*\n",
        "\n",
        "*Last edited: 28/11/2022*\n",
        "\n",
        "*Related paper: [Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models](https://www.medrxiv.org/content/XXXXXX)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialization"
      ],
      "metadata": {
        "id": "3yPy5XG9EhK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short string describing the current run"
      ],
      "metadata": {
        "id": "vFRsV3ecEkaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"bioasq4a_base_QA\""
      ],
      "metadata": {
        "id": "hfFXz7-vEjhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "7X2vfOs1Enn0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vRlGOXlrW2H"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# If running on colab, install first\n",
        "!pip3 install datasets transformers evaluate seqeval\n",
        "\n",
        "# Google Colab only\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "# General\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import cuda\n",
        "import os\n",
        "from io import StringIO\n",
        "import json\n",
        "from uuid import uuid4\n",
        "import time\n",
        "\n",
        "# HuggingFace Transformers\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback, set_seed\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence, DatasetDict, Features, Value, Sequence, ClassLabel, Dataset\n",
        "from evaluate import load\n",
        "\n",
        "# Set device to GPU Cuda if available \n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "input_file_path = \"BioASQ_4b_splitted.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Session info"
      ],
      "metadata": {
        "id": "8Y4TescoKs8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_info = json.loads(os.popen(\"curl curl ipinfo.io\").read())\n",
        "if device=='cuda':\n",
        "  gpu_info = pd.read_csv(StringIO(os.popen(\"nvidia-smi --query-gpu=gpu_name,memory.total --format=csv\").read()),names=[\"name\",\"memory\"],header=0)\n",
        "  session_info[f'gpus'] = [{'name': row[\"name\"], 'memory': row[\"memory\"]} for index, row in gpu_info.iterrows()] \n",
        "else: \n",
        "  session_info[f'gpus'] = []\n",
        "session_info['time_start'] = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "session_info['experiment_name'] = experiment_name\n",
        "session_info"
      ],
      "metadata": {
        "id": "hawyD8xh0xFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_yWIYH24sjG"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Input: json file with train, test, dev splits\n",
        "\n",
        "Output Expected:\n",
        "\n",
        "      {\n",
        "        {'answers:{\n",
        "            'answer_start': [(int)] -> the char position where the answer starts in the context, provided as an array\n",
        "            'text' [(str)] -> the answer as a string, provided as an array\n",
        "        }\n",
        "        'context': (str) --> the context as a string\n",
        "        'id': (str) --> unique id\n",
        "        'question': (str) --> the question as a string\n",
        "        'title': (str) --> the title of the source document as a string\n",
        "      }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig1VJF5FRadJ"
      },
      "source": [
        "Questions have been translated --> we've lost the char index --> get it back by searching the answer in the context --> get the first match if multiple.\n",
        "\n",
        "FUTURE IMPLEMENTATION: if original_char_index is available --> get the match that is closer to original_char_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRPf03Yc7dSv"
      },
      "outputs": [],
      "source": [
        "def get_answer_start(answer,context):\n",
        "  idx = None\n",
        "  try:\n",
        "    idx = context.index(answer)\n",
        "  except:\n",
        "    #automatic translation can generate uppercase first letter\n",
        "    try: \n",
        "      idx = context.index(answer[0].lower() + answer[1:])\n",
        "    except:\n",
        "      #attempt: lowercase everything\n",
        "      try: \n",
        "        idx = context.lower().index(answer.lower())\n",
        "      except:\n",
        "        #give up\n",
        "        idx = None\n",
        "        print(\"Warning: char index not found in provided context for '\"+answer+\"'\")\n",
        "        print(context)\n",
        "  return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKm1S2dcSFko"
      },
      "source": [
        "Parse the raw json to be compliant with the schema above. No info about documents provided --> replace document name with random ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPe51HSiN4lU"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from uuid import uuid4\n",
        "import os\n",
        "\n",
        "ENCODING = 'utf-8'\n",
        "\n",
        "def parse(file_path):\n",
        "    f = open(file_path, encoding=ENCODING)\n",
        "    json_data = json.load(f)\n",
        "    for split in [\"train\",\"test\",\"dev\"]:\n",
        "      output = {'data': []}\n",
        "      input = json_data[split]  # input to iterate through\n",
        "      id=0\n",
        "      for example in input:\n",
        "        output['data'].append({\n",
        "            'answers': {\n",
        "                'answer_start': [get_answer_start(example['answer'],example['context'])],\n",
        "                'text': [example['answer']]\n",
        "            },\n",
        "            'context': example['context'],\n",
        "            'id': str(uuid4()),\n",
        "            'question': example['question'],\n",
        "            'title': str(uuid4())\n",
        "        })\n",
        "      # Save file as a new json\n",
        "      with open(os.path.splitext(file_path)[0]+'-'+split+os.path.splitext(file_path)[1], 'w') as outfile:\n",
        "        json.dump(output, outfile)\n",
        "    # Closing file\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ7TEsGlSS72"
      },
      "source": [
        "Load parsed json as a Huggingface dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parse(input_file_path)\n",
        "dataset = load_dataset(\"json\", data_files={'train': f\"{os.path.splitext(input_file_path)[0]}-train.json\",\n",
        "                                           'test': f\"{os.path.splitext(input_file_path)[0]}-test.json\",\n",
        "                                           'dev': f\"{os.path.splitext(input_file_path)[0]}-dev.json\"}, field=\"data\")"
      ],
      "metadata": {
        "id": "XvYLfQS83zbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "kcp-1ZS65sU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AW_FJxBNoz7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKnY1LqFrfnV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoints = [\n",
        "    \"dbmdz/bert-base-italian-xxl-cased\", # Baseline\n",
        "    \"/content/gdrive/MyDrive/Colab Environments/biobert_models/bio-full\", # BioBERT\n",
        "    #\"/content/gdrive/MyDrive/Colab Environments/biobert_models/med-reg-v3\", # Best model w/o corpus augmentation\n",
        "    #\"/content/gdrive/MyDrive/Colab Environments/biobert_models/med-reg-v12\", # Best ER model w/ corpus augmentation\n",
        "    \"/content/gdrive/MyDrive/Colab Environments/biobert_models/med-reg-v3-enriched\" # Best MIXOUT model w/ corpus augmentation\n",
        "    ]\n",
        "seeds = [\n",
        "    #3407, \n",
        "    #6, \n",
        "    #11, \n",
        "    61, \n",
        "    #39\n",
        "    ]\n",
        "\n",
        "#This can be changed according to the downstream dataset. The only important thing is that they remain consistent for *ALL* the models  \n",
        "batch_size = 10\n",
        "learning_rate = 3e-5\n",
        "epochs=20\n",
        "weight_decay=0.01"
      ],
      "metadata": {
        "id": "N_YcWZCZC0dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define metrics"
      ],
      "metadata": {
        "id": "ztfuivw8GGbb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8IN-tUfUv3-"
      },
      "outputs": [],
      "source": [
        "#metric = load_metric(\"squad\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)\n",
        "\n",
        "def compute_metrics(p):\n",
        "  logits, labels = p\n",
        "  predictions = np.argmax(logits, axis=2)\n",
        "  legit_predictions = (predictions[1]-predictions[0]) >0\n",
        "  labels_cls = labels[0]!=0\n",
        "  tokens_diff = np.sum(predictions-labels, axis=0)\n",
        "  tot = len(tokens_diff)\n",
        "  tolerance = 10\n",
        "\n",
        "  exact_match = tokens_diff==0 #a.k.a strict accuracy\n",
        "  approx_match = abs(tokens_diff)<=tolerance\n",
        "\n",
        "  #accuracy = exact match of end tokens and start tokens\n",
        "  accuracy = np.sum(exact_match)/tot\n",
        "  #accuracy no cls = exact match without considering [CLS] tokens. Useful when we know that there will be >512 tokens long contexts \n",
        "  #doc stride will break context in subcontext and assign 0 as the expected prediction for subcontexts that do not include the answer\n",
        "  accuracy_not_cls = np.sum(np.logical_and(exact_match, labels_cls))/np.sum(labels_cls)\n",
        "  #accuracy_tol = predicted end tokens and start tokens are in close proximity with the actual ones\n",
        "  #we have to filter out the examples that pass the proximity condition but have pred end tokens < pred start tokens, because it is impossible\n",
        "  #this is automatically addressed in accuracy because it is an exact match\n",
        "  accuracy_approx = np.sum(np.logical_and(approx_match,legit_predictions))/tot\n",
        "\n",
        "  #pred_string = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[predictions[0]:predictions[1]]))\n",
        "  #true_string = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "  #return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
        "  return {'acc': accuracy, 'acc_no_cls': accuracy_not_cls, 'acc_approx': accuracy_approx}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA Trainer Class Definition"
      ],
      "metadata": {
        "id": "XbPgjHDjGIIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1o3mxi1UvRW"
      },
      "outputs": [],
      "source": [
        "from torch import argmax, softmax\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, default_data_collator, Trainer\n",
        "import re\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "# ==== PREDEFINED SETTINGS ====\n",
        "SEED = 666\n",
        "CHECKPOINT = 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa'\n",
        "BATCH_SIZE = 10\n",
        "MAX_LENGTH = 384  # The maximum length of a feature (question and context)\n",
        "DOC_STRIDE = 128  # The authorized overlap between two part of the context when splitting it is needed.\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 20\n",
        "PATIENCE = 4\n",
        "WEIGHT_DECAY = 0.01\n",
        "OUTPUT_DIR = \"model\"\n",
        "METRIC_BEST = \"acc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWi6ylKdUzAX"
      },
      "outputs": [],
      "source": [
        "# ==== TRAINER QA ====\n",
        "class TrainerQA(object):\n",
        "    \"\"\" This class provides utilities to train QA Transformers \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 checkpoint=CHECKPOINT,\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=EPOCHS,\n",
        "                 seed=SEED,\n",
        "                 learning_rate=LEARNING_RATE,\n",
        "                 weight_decay=WEIGHT_DECAY,\n",
        "                 output_dir=OUTPUT_DIR):\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "        self._model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
        "        self._args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            logging_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            save_total_limit=5,\n",
        "            load_best_model_at_end = True,\n",
        "            metric_for_best_model = METRIC_BEST,\n",
        "            learning_rate=learning_rate,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            per_device_eval_batch_size=batch_size,\n",
        "            num_train_epochs=epochs,\n",
        "            weight_decay=weight_decay,\n",
        "            seed=seed\n",
        "        )\n",
        "        self._data_collator = default_data_collator\n",
        "        self._tokenized_datasets = None\n",
        "        self.__debug_mode = False\n",
        "\n",
        "    def __prepare_train_features(self, examples, max_length=MAX_LENGTH, doc_stride=DOC_STRIDE):\n",
        "        pad_on_right = self._tokenizer.padding_side == \"right\"\n",
        "        # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\n",
        "        # context that overlaps a bit the context of the previous feature.\n",
        "        tokenized_examples = self._tokenizer(\n",
        "            examples[\"question\" if pad_on_right else \"context\"],\n",
        "            examples[\"context\" if pad_on_right else \"question\"],\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "            max_length=max_length,\n",
        "            stride=doc_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "        # its corresponding example. This key gives us just that.\n",
        "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "        # help us compute the start_positions and end_positions.\n",
        "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "        # Let's label those examples!\n",
        "        tokenized_examples[\"start_positions\"] = []\n",
        "        tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "        for i, offsets in enumerate(offset_mapping):\n",
        "            # We will label impossible answers with the index of the CLS token.\n",
        "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "            cls_index = input_ids.index(self._tokenizer.cls_token_id)\n",
        "\n",
        "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "            # One example can give several spans, this is the index of the example containing this span of text.\n",
        "            sample_index = sample_mapping[i]\n",
        "            answers = examples[\"answers\"][sample_index]\n",
        "            # If no answers are given, set the cls_index as answer.\n",
        "            if len(answers[\"answer_start\"]) == 0:\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Start/end character index of the answer in the text.\n",
        "                start_char = answers[\"answer_start\"][0]\n",
        "                end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "                # Start token index of the current span in the text.\n",
        "                token_start_index = 0\n",
        "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
        "                    token_start_index += 1\n",
        "\n",
        "                # End token index of the current span in the text.\n",
        "                token_end_index = len(input_ids) - 1\n",
        "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
        "                    token_end_index -= 1\n",
        "\n",
        "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "                else:\n",
        "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                        token_start_index += 1\n",
        "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                    while offsets[token_end_index][1] >= end_char:\n",
        "                        token_end_index -= 1\n",
        "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "        self._tokenizer            \n",
        "\n",
        "        return tokenized_examples\n",
        "\n",
        "    def load_dataset(self, dataset):\n",
        "        self._tokenized_datasets = dataset.map(self.__prepare_train_features, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "    def train(self, save_path):\n",
        "        trainer = Trainer(\n",
        "            self._model,\n",
        "            self._args,\n",
        "            train_dataset=self._tokenized_datasets[\"train\"],\n",
        "            eval_dataset=self._tokenized_datasets[\"dev\"],\n",
        "            data_collator=default_data_collator,\n",
        "            tokenizer=self._tokenizer,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks = [EarlyStoppingCallback(early_stopping_patience = PATIENCE)]\n",
        "        )\n",
        "        trainer.train()\n",
        "        trainer.save_model(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG5JKJgvU59X"
      },
      "outputs": [],
      "source": [
        "# ==== MODEL QA ====\n",
        "class ModelQA(object):\n",
        "    \"\"\" This class provides utilities to use QA Transformers \"\"\"\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        self._tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "        self._model = AutoModelForQuestionAnswering.from_pretrained(model_path, local_files_only=True)\n",
        "        self._shared_context = None\n",
        "        self.__debug_mode = False\n",
        "\n",
        "    @property\n",
        "    def tokenizer(self):\n",
        "        return self._tokenizer\n",
        "\n",
        "    @property\n",
        "    def model(self):\n",
        "        return self._model\n",
        "\n",
        "    @property\n",
        "    def shared_context(self):\n",
        "        return self._shared_context\n",
        "\n",
        "    @shared_context.setter\n",
        "    def shared_context(self, value):\n",
        "        self._shared_context = value\n",
        "\n",
        "    # Methods\n",
        "    def toggle_debug(self):\n",
        "        self.__debug_mode = not self.__debug_mode\n",
        "\n",
        "    def ask_question_stride(self, question=\"\", context=None, print_results=True, max_length=MAX_LENGTH, doc_stride=DOC_STRIDE):\n",
        "        pad_on_right = self._tokenizer.padding_side == \"right\"\n",
        "        # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "        # in one example possible giving several features when a context is long, each of those features having a\n",
        "        # context that overlaps a bit the context of the previous feature.\n",
        "        tokenized_examples = self._tokenizer(\n",
        "            question if pad_on_right else context,\n",
        "            context if pad_on_right else question,\n",
        "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "            max_length=max_length,\n",
        "            stride=doc_stride,\n",
        "            add_special_tokens=True,\n",
        "            return_overflowing_tokens=True,\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        answers = {'answer': [], 'probs': [], 'indices': []}\n",
        "        for i in range(len(tokenized_examples['input_ids'])):\n",
        "          inputs = {}\n",
        "          for key in ['input_ids', 'token_type_ids', 'attention_mask']:\n",
        "            inputs[key] = stack([tokenized_examples[key][i]])\n",
        "\n",
        "          input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "          #print(inputs[\"input_ids\"].tolist())\n",
        "          text_tokens = self._tokenizer.convert_ids_to_tokens(input_ids)  # Very useful for debug!!!\n",
        "          outputs = self._model(**inputs)\n",
        "          answer_start_scores = outputs.start_logits\n",
        "          # print(answer_start_scores)\n",
        "          # support_start = answer_start_scores.tolist()\n",
        "          answer_end_scores = outputs.end_logits\n",
        "          answer_start = argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
        "          answer_end = argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "          # WARNING: convert_tokens_to_string automatically adds \" \" after special tokens! e.g. 1.2 mV --> 1. 2 mV\n",
        "          answer = self._tokenizer.convert_tokens_to_string(self._tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "          # Get % of start and end tokens\n",
        "          answer_start_list = softmax(answer_start_scores, dim=1).tolist()[0]\n",
        "          start_index, start_prob = max(enumerate(answer_start_list), key=lambda x: x[1])\n",
        "          answer_end_list = softmax(answer_end_scores, dim=1).tolist()[0]\n",
        "          end_index, end_prob = max(enumerate(answer_end_list), key=lambda x: x[1])\n",
        "\n",
        "          if print_results:\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'Start token probability: {start_prob:.2f}')\n",
        "            print(f'End token probability: {end_prob:.2f}')\n",
        "          else:\n",
        "            answers[\"answer\"].append(answer)\n",
        "            answers[\"probs\"].append([start_prob, end_prob])\n",
        "            answers[\"indices\"].append([int(answer_start), int(answer_end)])\n",
        "        return answers\n",
        "\n",
        "    def ask_question(self, question=\"\", context=None, print_results=True, return_topK = 0):\n",
        "        if context is None:\n",
        "            if self._shared_context is None:\n",
        "                raise Exception(\"Shared context not provided\")\n",
        "            else:\n",
        "                context = self._shared_context\n",
        "\n",
        "        inputs = self._tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\", truncation=True)\n",
        "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "        # print(inputs[\"input_ids\"].tolist())\n",
        "        text_tokens = self._tokenizer.convert_ids_to_tokens(input_ids)  # Very useful for debug!!!\n",
        "        outputs = self._model(**inputs)\n",
        "        answer_start_scores = outputs.start_logits\n",
        "        # print(answer_start_scores)\n",
        "        # support_start = answer_start_scores.tolist()\n",
        "        answer_end_scores = outputs.end_logits\n",
        "        answer_start = argmax(answer_start_scores)  # Get the most likely beginning of answer with the argmax of the score\n",
        "        answer_end = argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "\n",
        "        ### DEBUG ##############\n",
        "\n",
        "        # ### <TEST> return the top k answers (i.e. the top k couples that maximizes the sum between beginning and end token)\n",
        "        topk_indices = None\n",
        "\n",
        "        # if return_topK>0:\n",
        "        #   #get the most likely beginnings and ends (top k)\n",
        "        #   ind_starts = np.argpartition(answer_start_scores,-return_topK)[-return_topK:] #top K most likely beginnings\n",
        "        #   ind_ends = np.argpartition(answer_end_scores,-return_topK)[-return_topK:] #top K most likely ends\n",
        "\n",
        "        #   #get the correspondent confidence score\n",
        "        #   pred_starts = answer_start_scores[ind_starts]\n",
        "        #   pred_ends = answer_end_scores[ind_ends]\n",
        "\n",
        "        #   #link the two\n",
        "        #   dict_starts = dict(zip(ind_starts,pred_starts))\n",
        "        #   dict_ends = dict(zip(ind_ends,pred_ends))\n",
        "\n",
        "        #   #get all the possible combinations of the beginnings end ends\n",
        "        #   indlist = list(product(ind_starts,ind_ends))\n",
        "\n",
        "        #   #from this set of combinations, keep only the ones where start<=end\n",
        "        #   indlist_ok = [x for x in indlist if x[0]<=x[1]]\n",
        "\n",
        "        #   #get the sum of the probabilities for each couple of beginning and ends\n",
        "        #   sums = [dict_starts[x[0]]+dict_ends[x[1]] for x in indlist_ok]\n",
        "\n",
        "        #   #sort by max sum and keep top k\n",
        "        #   indmax_k = np.argsort(sums)[::-1][:return_topK]\n",
        "\n",
        "        #   #get the top k couples of begginings and ends that maximizes the sum\n",
        "        #   topk_indices = [indlist_ok[i] for i in indmax_k]\n",
        "\n",
        "        ### DEBUG ##############\n",
        "\n",
        "\n",
        "        # WARNING: convert_tokens_to_string automatically adds \" \" after special tokens! e.g. 1.2 mV --> 1. 2 mV\n",
        "        answer = self._tokenizer.convert_tokens_to_string(self._tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "        # Get % of start and end tokens\n",
        "        answer_start_list = softmax(answer_start_scores, dim=1).tolist()[0]\n",
        "        start_index, start_prob = max(enumerate(answer_start_list), key=lambda x: x[1])\n",
        "        answer_end_list = softmax(answer_end_scores, dim=1).tolist()[0]\n",
        "        end_index, end_prob = max(enumerate(answer_end_list), key=lambda x: x[1])\n",
        "\n",
        "        if print_results:\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'Start token probability: {start_prob:.2f}')\n",
        "            print(f'End token probability: {end_prob:.2f}')\n",
        "        else:\n",
        "            return {'answer': answer, 'probs': [start_prob, end_prob], 'indices': [int(answer_start), int(answer_end)], 'top_k':topk_indices}\n",
        "\n",
        "    #   tokens --> lower text and remove punctuation (ENGLISH!), articles and extra whitespace\n",
        "    #   precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    #   recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    #   f1 = (2 * precision * recall) / (precision + recall)\n",
        "    #   exact_match = prediction == ground_truth\n",
        "    #   warning: iterate over single prediction-reference pairs, do not pass the whole test set to compute! Flawed somewhere\n",
        "    def validate(self, dataset):\n",
        "        squad_metric = load(\"squad\")\n",
        "        formatted_predictions = [\n",
        "            {\"id\": str(ex['id']),\n",
        "             \"prediction_text\": re.sub(\"(?<=\\\\d(\\\\.|\\\\,))\\\\s+(?=\\\\d)\", \"\", self.ask_question(ex['question'], ex['context'], print_results=False)['answer'])} for ex in dataset]\n",
        "        references = [{\"id\": str(ex['id']), \"answers\": ex['answers']} for ex in dataset]\n",
        "        return {\n",
        "            'predictions': formatted_predictions,\n",
        "            'references': references,\n",
        "            'metric': squad_metric.compute(predictions=formatted_predictions, references=references)\n",
        "        }\n",
        "\n",
        "\n",
        "    ########DEBUG\n",
        "\n",
        "    # #strict accuracy, lenient accuracy, mean reciprocal rank\n",
        "    # def validate2(self, dataset):\n",
        "    #     metric = load_metric(\"squad\")\n",
        "\n",
        "    #     #1) strict accuracy (if tol = 0)\n",
        "    #     def idx_match(reference, prediction,tolerance=0):\n",
        "    #       ref_start = reference[\"answer_start\"][0]\n",
        "    #       ref_end = ref_start+len(reference[\"text\"][0])\n",
        "    #       pred_start = prediction[0]\n",
        "    #       pred_end = prediction[1]\n",
        "    #       tokens_diff = (ref_start-pred_start)+(ref_end-pred_end)\n",
        "    #       match = abs(tokens_diff)<=tolerance\n",
        "    #     matches = [idx_match(self.ask_question(ex['question'], ex['context'], print_results=False)['indices'],\n",
        "    #                          ex['answers'],\n",
        "    #                          tolerance=0) for ex in dataset]\n",
        "    #     strict_acc = np.sum(matches)/len(matches)\n",
        "\n",
        "    #     #2) lenient accuracy\n",
        "    #     #if there's a match in at least one of the top k cases\n",
        "    #     def idx_match_k(reference, top_ks, tolerance=0):\n",
        "    #       ref_start = reference[\"answer_start\"][0]\n",
        "    #       ref_end = ref_start+len(reference[\"text\"][0])\n",
        "    #       match = False\n",
        "    #       for top_k in top_ks:\n",
        "    #         pred_start = top_k[0]\n",
        "    #         pred_end = top_k[1]\n",
        "    #         tokens_diff = (ref_start-pred_start)+(ref_end-pred_end)\n",
        "    #         if abs(tokens_diff)<=tolerance:\n",
        "    #           match = True\n",
        "    #       return match\n",
        "    #     matches = [idx_match_k(self.ask_question(ex['question'], ex['context'], print_results=False, return_topK=5)['top_k'],\n",
        "    #                          ex['answers'],\n",
        "    #                          tolerance=0) for ex in dataset]\n",
        "    #     lenient_acc = np.sum(matches)/len(matches)\n",
        "\n",
        "    #     #3) MRR\n",
        "        \n",
        "    #     def idx_match_mrr(reference, top_ks, tolerance=0):\n",
        "    #       ref_start = reference[\"answer_start\"][0]\n",
        "    #       ref_end = ref_start+len(reference[\"text\"][0])\n",
        "    #       match_rank = 0\n",
        "    #       for i in range(len(top_ks)):\n",
        "    #         top_k = top_ks[i]\n",
        "    #         pred_start = top_k[0]\n",
        "    #         pred_end = top_k[1]\n",
        "    #         tokens_diff = (ref_start-pred_start)+(ref_end-pred_end)\n",
        "    #         if abs(tokens_diff)<=tolerance:\n",
        "    #           match_rank = i\n",
        "    #           break\n",
        "    #       if match_rank==0:\n",
        "    #         return 0\n",
        "    #       else:\n",
        "    #         return 1/match_rank\n",
        "    #     matches = [idx_match_mrr(self.ask_question(ex['question'], ex['context'], print_results=False, return_topK=5)['top_k'],\n",
        "    #                          ex['answers'],\n",
        "    #                          tolerance=0) for ex in dataset]\n",
        "    #     mrr = np.sum(matches)/len(matches)       \n",
        "\n",
        "    #     return {\n",
        "    #         'strict_acc': strict_acc,\n",
        "    #         'lenient_acc': lenient_acc,\n",
        "    #         'mrr': metric.compute(predictions=predictions, references=references)\n",
        "    #     }\n",
        "    ########DEBUG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ2BZeJkU912"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil  \n",
        "\n",
        "for model_checkpoint in model_checkpoints:\n",
        "  df_results = pd.DataFrame(columns= ['f1', 'exact_match', 'seed'])\n",
        "  for seed in seeds:\n",
        "    # Seed must be set before creating the model, otherwise the random head will be initialized in a different way every time and the results will not be replicable\n",
        "    # From now on, the seed is set for *all* the random processes, including numpy, sklearn, etc...not only for transformers!\n",
        "    set_seed(seed)\n",
        "\n",
        "    output_dir = f\"/content/{os.path.basename(model_checkpoint)}_ft_QA/{seed}\"\n",
        "    trainer = TrainerQA(checkpoint=model_checkpoint, seed=seed, output_dir=output_dir)\n",
        "    trainer.load_dataset(dataset)\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train(output_dir)\n",
        "\n",
        "    # Collect results on test set\n",
        "    model = ModelQA(model_path=output_dir)\n",
        "    results = model.validate(dataset['test'])\n",
        "    row = {'f1': results['metric']['f1'],'exact_match': results['metric']['exact_match'], 'seed':seed}\n",
        "    df_results = df_results.append(row, ignore_index=True)\n",
        "    display(df_results)\n",
        "\n",
        "  display(df_results)\n",
        "  df_results.to_csv(f'/content/results_{os.path.basename(model_checkpoint)}.csv')\n",
        "  files.download(f'/content/results_{os.path.basename(model_checkpoint)}.csv')\n",
        "\n",
        "  #Free up memory for next checkpoint iteration\n",
        "  shutil.rmtree(f\"/content/{os.path.basename(model_checkpoint)}_ft_QA\",ignore_errors=True)"
      ],
      "metadata": {
        "id": "6oL4QjUuG20L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalize session info and download"
      ],
      "metadata": {
        "id": "DgwaQ7uITFXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_info['checkpoints'] = [os.path.basename(c) for c in model_checkpoints]\n",
        "session_info['seeds'] = seeds\n",
        "session_info['training_arguments'] = []\n",
        "session_info['time_end'] = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "\n",
        "with open(f'/content/session_info.json', \"w\") as outfile:\n",
        "    outfile.write(json.dumps(session_info, indent=4))\n",
        "files.download(f'/content/session_info.json')"
      ],
      "metadata": {
        "id": "NNNE8WGBR7m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47gH_C9PVsZf"
      },
      "source": [
        "# Interactive (optional)\n",
        "\n",
        "Remove `%%script echo skipping` to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv3_RIwYVuce"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "import ipywidgets as widgets\n",
        "def ask(context, question):\n",
        "  context_size = len(model._tokenizer(question, context)[\"input_ids\"])\n",
        "  print(context_size)\n",
        "  if context_size>512:\n",
        "    answer = model.ask_question_stride(question, context, print_results=False)\n",
        "  else:\n",
        "    answer = model.ask_question(context,question, print_results=False)\n",
        "  return (answer[\"answer\"], answer[\"probs\"])\n",
        "int_widget = widgets.interact_manual(ask, \n",
        "                                     context=widgets.Textarea('Context', layout=widgets.Layout(width='70%', height='100px')),\n",
        "                                     question=widgets.Text('Question', layout=widgets.Layout(width='70%')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unassign runtime to avoid wasting compute units"
      ],
      "metadata": {
        "id": "T5g7quv2KaJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR49228sSl3-"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}