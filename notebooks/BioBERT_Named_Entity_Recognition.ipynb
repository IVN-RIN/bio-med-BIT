{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **BioBERT ITA Fine-Tuning Experiment For <u>Named Entity Recognition</u>**\n",
        "\n",
        "*Tommaso Buonocore, University of Pavia, 2022*\n",
        "\n",
        "*Last edited: 16/11/2022*"
      ],
      "metadata": {
        "id": "jOwFciEII3RH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialization"
      ],
      "metadata": {
        "id": "uc8aw7YEKfk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short string describing the current run"
      ],
      "metadata": {
        "id": "oJqVXR07EbGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = \"Species800-NER\""
      ],
      "metadata": {
        "id": "vBeeySEHESey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports"
      ],
      "metadata": {
        "id": "FGANZi0RKroP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPnpQAV7R5Jf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# If running on colab, install first\n",
        "!pip3 install datasets transformers seqeval\n",
        "\n",
        "# Google Colab only\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "# General\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch import cuda\n",
        "import json\n",
        "import os\n",
        "from io import StringIO\n",
        "import time\n",
        "\n",
        "# HuggingFace Transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback, set_seed\n",
        "from datasets import load_dataset, load_metric, ClassLabel, Sequence, DatasetDict, Features, Value, Sequence, ClassLabel, Dataset\n",
        "\n",
        "# Set device to GPU Cuda if available \n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Session info"
      ],
      "metadata": {
        "id": "8Y4TescoKs8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_info = json.loads(os.popen(\"curl curl ipinfo.io\").read())\n",
        "if device=='cuda':\n",
        "  gpu_info = pd.read_csv(StringIO(os.popen(\"nvidia-smi --query-gpu=gpu_name,memory.total --format=csv\").read()),names=[\"name\",\"memory\"],header=0)\n",
        "  session_info[f'gpus'] = [{'name': row[\"name\"], 'memory': row[\"memory\"]} for index, row in gpu_info.iterrows()] \n",
        "else: \n",
        "  session_info[f'gpus'] = []\n",
        "session_info['time_start'] = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "session_info['experiment_name'] = experiment_name\n",
        "session_info"
      ],
      "metadata": {
        "id": "hawyD8xh0xFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "EXLRmwLNK3l9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we expect to have six files already loaded in the current session (by drag and drop):\n",
        "\n",
        "*   \"/content/*_corpus.txt\" : document id and document string, separated by \"|\"\n",
        "*   \"/content/*_annotations.txt\": details about the annotation (e.g., start index, end index, value, tag label, etc.) separated by \"/t\"\n",
        "\n",
        "(* = \"train\", \"test\", \"dev\")\n",
        "\n"
      ],
      "metadata": {
        "id": "k2FddOi7K-3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G1odm-L1UqQ"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "def prepare_dataset(ds_corpus, ds_tags):\n",
        "  tokenizer = WordPunctTokenizer()\n",
        "  dropped = 0\n",
        "  taglist = [[]]*len(ds_corpus)\n",
        "  for i in range(len(ds_corpus)):\n",
        "    row = ds_corpus.iloc[i,:]\n",
        "    id=row[\"id\"]\n",
        "    text=tokenizer.tokenize(row[\"tokens\"])\n",
        "    tags = [\"O\"]*len(text)\n",
        "    filtered_rows = ds_tags[ds_tags['id'] == id]\n",
        "    for j in range(len(filtered_rows)):\n",
        "      row = filtered_rows.iloc[j,:]\n",
        "      word = row[\"value\"]\n",
        "      if word in text:\n",
        "        idx = text.index(word)\n",
        "        tags[idx] = \"B-\"+row[\"ner_tags\"]\n",
        "      else:\n",
        "        #tokenize word into ...words, because it's probably multi word\n",
        "        #look into tokens to find the position of the corresponding sequence\n",
        "        #associate \"B-TAG\" to the first one and \"I-TAG\" to the others\n",
        "        tword = tokenizer.tokenize(word)\n",
        "        idxs = [(i, i+len(tword)) for i in range(len(text)) if text[i:i+len(tword)] == tword]\n",
        "        if len(idxs)>0:\n",
        "          tags[idxs[0][0]] = \"B-\"+row[\"ner_tags\"]\n",
        "          for k in range(idxs[0][0]+1,idxs[0][1]):\n",
        "            tags[k] = \"I-\"+row[\"ner_tags\"]\n",
        "        else:\n",
        "          dropped  +=1\n",
        "    taglist[i]=tags\n",
        "  print(f\"{round(dropped*100/len(ds_tags),2)}% dropped\")\n",
        "\n",
        "  ds_corpus[\"ner_tags\"] = taglist\n",
        "  ds_corpus[\"tokens\"] = [tokenizer.tokenize(ds_corpus.iloc[i,:][\"tokens\"]) for i in range(len(ds_corpus))]\n",
        "\n",
        "  dataset = Dataset.from_pandas(ds_corpus)\n",
        "\n",
        "  class_names = [*['O'],\n",
        "               *['B-'+tag for tag in set(ds_tags[\"ner_tags\"])],\n",
        "               *['I-'+tag for tag in set(ds_tags[\"ner_tags\"])]]\n",
        "  dataset_features = Features({'id': Value('string'), 'tokens': Sequence(feature=Value('string')), 'ner_tags': Sequence(feature=ClassLabel(names=class_names))})\n",
        "  dataset = dataset.map(dataset_features.encode_example, features=dataset_features)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk0nURYnt7X1"
      },
      "outputs": [],
      "source": [
        "default_missing = pd._libs.parsers.STR_NA_VALUES\n",
        "default_missing = default_missing.remove('NA') #sodium (NA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N09VWS416uL"
      },
      "outputs": [],
      "source": [
        "#colnames = [\"id\",\"type\",\"start\",\"end\",\"value\",\"ner_tags\"]\n",
        "colnames = [\"id\",\"start\",\"end\",\"value\",\"ner_tags\"]\n",
        "\n",
        "ds_tags = pd.read_csv('/content/train_annotations.txt', sep = '\\t', header=None, names = [\"id\",\"start\",\"end\",\"value\",\"ner_tags\"], index_col=False,na_values=default_missing)\n",
        "print(f'tags: {set(ds_tags[\"ner_tags\"])}')\n",
        "\n",
        "ds_corpus = pd.read_csv('/content/train_corpus.txt', sep ='|', header=None, names = [\"id\", \"tokens\"],index_col=False)\n",
        "ds_tags = pd.read_csv('/content/train_annotations.txt', sep = '\\t', header=None, names = colnames, na_values=default_missing,index_col=False)\n",
        "dtrain = prepare_dataset(ds_corpus, ds_tags)\n",
        "\n",
        "ds_corpus = pd.read_csv('/content/test_corpus.txt', sep ='|', header=None, names = [\"id\", \"tokens\"],index_col=False)\n",
        "ds_tags = pd.read_csv('/content/test_annotations.txt', sep = '\\t', header=None, names = colnames, na_values=default_missing,index_col=False)\n",
        "dtest = prepare_dataset(ds_corpus, ds_tags)\n",
        "\n",
        "ds_corpus = pd.read_csv('/content/dev_corpus.txt', sep ='|', header=None, names = [\"id\", \"tokens\"],index_col=False)\n",
        "ds_tags = pd.read_csv('/content/dev_annotations.txt', sep = '\\t', header=None, names = colnames, na_values=default_missing,index_col=False)\n",
        "ddev = prepare_dataset(ds_corpus, ds_tags)\n",
        "\n",
        "# Split the train + valid in 80% train, %20 valid\n",
        "# train_valid = dtrain.train_test_split(test_size=0.20, seed=seed)\n",
        "\n",
        "# gather everyone if you want to have a single DatasetDict\n",
        "datasets = DatasetDict({\n",
        "    'train': dtrain,\n",
        "    'test': dtest,\n",
        "    'valid': ddev}) \n",
        "# Check the levels defined for the NER tagging task in this dataset.\n",
        "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJFIMSMKuuNF"
      },
      "outputs": [],
      "source": [
        "print(datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that will be called in the training loop"
      ],
      "metadata": {
        "id": "EDIYC74SQuQx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7JhzelNhif9"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, tokenizer, label_all_tokens = True):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on label_all_tokens.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "O2BGDVCBQRzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh2_IjJ9tUy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175e79ec-313c-44c0-8408-60b9aa1991cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sFPbve3Py05"
      },
      "outputs": [],
      "source": [
        "model_checkpoints = [\n",
        "    #\"dbmdz/bert-base-italian-xxl-cased\", # Baseline\n",
        "    #\"/content/gdrive/MyDrive/Colab Environments/biobert_models/bio-full\", # BioBERT\n",
        "    \"/content/gdrive/MyDrive/Colab Environments/biobert_models/med-reg-v3\", # Best model w/o corpus augmentation\n",
        "    #\"/content/gdrive/MyDrive/Colab Environments/biobert_models/med-reg-v12\", # Best ER model w/ corpus augmentation\n",
        "    #\"/content/gdrive/MyDrive/Colab Environments/biobert_models/med-reg-v3-enriched\" # Best MIXOUT model w/ corpus augmentation\n",
        "    ]\n",
        "seeds = [\n",
        "    3407, \n",
        "    6, \n",
        "    11, \n",
        "    61, \n",
        "    39\n",
        "    ]\n",
        "\n",
        "#This can be changed according to the downstream dataset. The only important thing is that they remain consistent for *ALL* the models    \n",
        "batch_size = 10\n",
        "learning_rate = 3e-5\n",
        "epochs=15\n",
        "weight_decay=0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define metrics"
      ],
      "metadata": {
        "id": "IOOm8YC2QYKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels, scheme=\"IOB2\", zero_division=1)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "a7wHcTuKQXL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ],
      "metadata": {
        "id": "tpZDqfnjQbfX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PACYPB5sNJK"
      },
      "outputs": [],
      "source": [
        "for model_checkpoint in model_checkpoints:\n",
        "  df_results = pd.DataFrame(columns= ['type', 'number', 'f1', 'precision','recall','seed'])\n",
        "  for seed in seeds:\n",
        "    # Seed must be set before creating the model, otherwise the random head will be initialized in a different way every time and the results will not be replicable\n",
        "    # From now on, the seed is set for *all* the random processes, including numpy, sklearn, etc...not only for transformers!\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Load tokenizer and initialize the TokenClassification transformer with checkpoint weights\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)                \n",
        "    tokenized_datasets = datasets.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=True)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "\n",
        "    # Define the training details\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = f\"/content/{os.path.basename(model_checkpoint)}_ft_NER/{seed}\",\n",
        "        evaluation_strategy = \"epoch\",\n",
        "        save_strategy = \"epoch\",\n",
        "        save_total_limit = 3,\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"f1\",\n",
        "        learning_rate = learning_rate,\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        per_device_eval_batch_size = batch_size,\n",
        "        num_train_epochs = epochs,\n",
        "        weight_decay = weight_decay,\n",
        "    )\n",
        "    data_collator = DataCollatorForTokenClassification(tokenizer, label_pad_token_id = -100, return_tensors =\"pt\") \n",
        "    metric = load_metric(\"seqeval\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        training_args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"valid\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks = [EarlyStoppingCallback(early_stopping_patience = 4)]\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    trainer.evaluate()\n",
        "\n",
        "    # Collect results on test set\n",
        "    predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels, scheme=\"IOB2\", zero_division=1)\n",
        "    print(results)\n",
        "\n",
        "    for key, value in results.items():\n",
        "      if 'overall' not in key:\n",
        "        row = {'type': key, 'number': value['number'], 'f1': value['f1'], 'precision': value['precision'], 'recall': value['recall'], 'seed': seed}\n",
        "        df_results = df_results.append(row, ignore_index=True)\n",
        "    row = {'type': 'overall','number': 0, 'f1': results['overall_f1'], 'precision': results['overall_precision'], 'recall': results['overall_recall'], 'seed':seed}\n",
        "    df_results = df_results.append(row, ignore_index=True)\n",
        "  \n",
        "  display(df_results)\n",
        "  df_results.to_csv(f'/content/results_{os.path.basename(model_checkpoint)}.csv')\n",
        "  files.download(f'/content/results_{os.path.basename(model_checkpoint)}.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalize session info and download"
      ],
      "metadata": {
        "id": "DgwaQ7uITFXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_info['checkpoints'] = [os.path.basename(c) for c in model_checkpoints]\n",
        "session_info['seeds'] = seeds\n",
        "session_info['training_arguments'] = training_args\n",
        "session_info['time_end'] = time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "\n",
        "with open(f'/content/session_info.json', \"w\") as outfile:\n",
        "    outfile.write(json.dumps(session_info, indent=4))\n",
        "files.download(f'/content/session_info.json')"
      ],
      "metadata": {
        "id": "NNNE8WGBR7m3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "eda5e0db-99d1-4dd1-a3ca-985c5862239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_40b536fc-0836-4995-9767-bbc3c6f3cc76\", \"session_info.json\", 628)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#uncomment to download the model for later use\n",
        "\n",
        "#!zip -r checkpoint.zip model_checkpoint-finetuned-NER/checkpoint-100\n",
        "#files.download('checkpoint.zip') #otherwise, right click on the zip file in the file system"
      ],
      "metadata": {
        "id": "KIUfIAxEsdW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ###QUALITATIVE EVALUATION\n",
        "# from collections import Counter\n",
        "# def pack_tokens_and_predictions(word_tokens, predictions, labels):\n",
        "#   tok_input = tokenizer(word_tokens, is_split_into_words=True)\n",
        "#   word_ids = tok_input.word_ids()\n",
        "#   c = Counter(word_ids)\n",
        "#   token_index = 1\n",
        "#   word_index = 0\n",
        "#   words = []\n",
        "#   preds = []\n",
        "#   while token_index<511:\n",
        "#     w_tokens = c.get(word_index)\n",
        "#     if w_tokens==None:\n",
        "#       break\n",
        "#     start = token_index\n",
        "#     end = token_index+w_tokens\n",
        "#     w_labels = labels[start:end]\n",
        "#     w_pred = predictions[start:end]\n",
        "#     maj_key_pred, maj_value_pred = Counter(w_pred).most_common()[0]\n",
        "#     coverage_pred = maj_value_pred/w_tokens\n",
        "#     coverage_string = \"\"\n",
        "#     if coverage_pred<1:\n",
        "#       coverage_string = \" (\"+str([tag_labels[w] for w in w_pred])+\")\"\n",
        "#     wrong_string = \"\"\n",
        "#     if maj_key_pred!=w_labels[0]:\n",
        "#       wrong_string = \" , expected \"+tag_labels[w_labels[0]]\n",
        "#     arrow = \"   \"\n",
        "#     if w_labels[0] != 0 or maj_key_pred != 0:\n",
        "#       arrow = \"-->\"\n",
        "#     word = tokenizer.decode(tok_input.input_ids[start:end])\n",
        "#     print(arrow+\"'\"+word+\"': \"+tag_labels[maj_key_pred]+coverage_string+wrong_string)\n",
        "#     token_index = end\n",
        "#     word_index += 1\n",
        "#     words.append(word)\n",
        "#     preds.append(tag_labels[maj_key_pred])\n",
        "#   return words, preds\n",
        "\n",
        "\n",
        "# tag_labels = datasets[\"test\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "# id_example_test = 4\n",
        "# words, preds = pack_tokens_and_predictions(tokenized_datasets[\"test\"][id_example_test][\"tokens\"], predictions[id_example_test], labels[id_example_test])\n",
        "\n",
        "# import IPython\n",
        "# color_palette=[\"#2fbbab\",\"#fd9720\",\"#a6e22d\",\"#ef60b4\",\"#F7DEA7\",\"#B0B4D1\",\"#ABC5F5\",\"#D9E5AE\",\"#F9C0C0\"]\n",
        "# colors = dict(zip(set(ds_tags[\"ner_tags\"]), color_palette[0:len(set(ds_tags[\"ner_tags\"]))]))\n",
        "# for i in range(len(words)):\n",
        "#   if(preds[i]!=\"O\"):\n",
        "#     color = colors[preds[i][2:]]\n",
        "#     tag = preds[i][2:5]\n",
        "#     js_code = f'''var container = document.querySelector(\"#output-area\");\n",
        "#               var span = document.createElement(\"span\");\n",
        "#               span.style.backgroundColor = \"{color}\"; \n",
        "#               span.style.borderRadius = \"0.25em\";\n",
        "#               span.style.lineHeight = \"2\";\n",
        "#               span.style.margin = \"0.1em 0.1em\";\n",
        "#               span.style.padding = \"0.3em 0.2em\";\n",
        "#               span.appendChild(document.createTextNode(\"{words[i]}\"));\n",
        "#               var tag = document.createElement(\"span\");\n",
        "#               tag.style.backgroundColor = \"rgba(255, 255, 255, 0.8)\";\n",
        "#               tag.style.borderRadius = \"0.25em\";\n",
        "#               tag.style.lineHeight = \"1\";\n",
        "#               tag.style.fontSize=\"xx-small\";\n",
        "#               tag.style.margin = \"0.1em 0.1em\";\n",
        "#               tag.style.padding = \"0.05em 0.1em\";\n",
        "#               tag.appendChild(document.createTextNode(\"{tag}\"));\n",
        "#               span.appendChild(tag);\n",
        "#               container.appendChild(span);'''\n",
        "#   else:\n",
        "#     text = words[i]+\" \"\n",
        "#     js_code = f'''document.querySelector(\"#output-area\").appendChild(document.createTextNode(\"{text}\"));'''\n",
        "#   display(IPython.display.Javascript(js_code))"
      ],
      "metadata": {
        "id": "cNKjXJ7MpEaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}